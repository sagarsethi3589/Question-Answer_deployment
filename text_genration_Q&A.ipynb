{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7478770,"sourceType":"datasetVersion","datasetId":4353240}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n**Statement:- We are supposed to produce new content like text generation Using LLM's**   \nWe will be using GPT-2 model to solve this problem and create more contents out of it.\n","metadata":{}},{"cell_type":"markdown","source":"**First We will read the dataset and extract all the information out of it in a variable, then we will create our mode to train it using the above data, after succesfully creating and saving the model we will test it on certain sets of questions recieved and afterour model is ready. We will need to deploy it, so for that very purpose we will use pickle for that.**","metadata":{}},{"cell_type":"code","source":"!pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:31:23.600281Z","iopub.execute_input":"2024-01-27T20:31:23.600762Z","iopub.status.idle":"2024-01-27T20:31:37.133806Z","shell.execute_reply.started":"2024-01-27T20:31:23.600728Z","shell.execute_reply":"2024-01-27T20:31:37.132615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading and reading the dataset","metadata":{}},{"cell_type":"code","source":"import PyPDF2\n\ndef extract_text_from_pdf(pdf_path):\n    # Open the PDF file in binary mode\n    with open(pdf_path, 'rb') as file:\n        # Create a PDF file reader object\n        pdf_reader = PyPDF2.PdfReader(file)\n        \n        # Initialize an empty string to store the extracted text\n        text = \"\"\n        \n        # Loop through each page in the PDF\n        for page_num in range(len(pdf_reader.pages)):\n            # Extract text from the current page\n            page = pdf_reader.pages[page_num]\n            text += page.extract_text()\n        \n        return text\n\n# Path to the PDF file\npdf_path = \"/kaggle/input/new-data/48lawsofpower.pdf\"\n\n# Extract text from the PDF\nextracted_text = extract_text_from_pdf(pdf_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:31:37.135855Z","iopub.execute_input":"2024-01-27T20:31:37.136244Z","iopub.status.idle":"2024-01-27T20:31:37.777916Z","shell.execute_reply.started":"2024-01-27T20:31:37.136207Z","shell.execute_reply":"2024-01-27T20:31:37.776948Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re\nextracted_text.split('\\n')\nextracted_text1 = re.sub('[^A-Za-z]',' ',extracted_text)\nextracted_text1.strip()\nextracted_text1 = re.sub(r\"\\s+\", \" \", extracted_text1.lower())\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:31:37.780174Z","iopub.execute_input":"2024-01-27T20:31:37.781116Z","iopub.status.idle":"2024-01-27T20:31:37.795316Z","shell.execute_reply.started":"2024-01-27T20:31:37.781082Z","shell.execute_reply":"2024-01-27T20:31:37.794551Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Training Of Data ","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2-medium\"  # Choose the desired model size\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\ntraining_text = []\ntraining_text.append(extracted_text1)\n# Add a padding token to the tokenizer\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the story data\ntokenized_data = tokenizer(training_text, return_tensors=\"pt\", padding=True, truncation=True)\n\n# Create a dataset from the tokenized data\ndataset = Dataset.from_dict(tokenized_data)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-finetuned\",\n    overwrite_output_dir=True,\n    num_train_epochs=100,\n    per_device_train_batch_size=8,\n    save_steps=10_000,\n    save_total_limit=2,\n)\n\n# Define the trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    train_dataset=dataset,\n)\n\n# Train the model\ntrainer.train()\ntrainer.save_model(\"./gpt2-finetuned\")\ntokenizer.save_pretrained(\"./my_tokenizer\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:31:37.796392Z","iopub.execute_input":"2024-01-27T20:31:37.796635Z","iopub.status.idle":"2024-01-27T20:44:34.262481Z","shell.execute_reply.started":"2024-01-27T20:31:37.796614Z","shell.execute_reply":"2024-01-27T20:44:34.261342Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818ed9ed4cb54337925c23cd2be9c433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55fdac992b6489786168558b7bea11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d71d9dd2bb84f3c8392849bff156847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674b10ac8c7e4453879b9a5378e9d604"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeffd086f1fc420bb807bc0e119d0836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be91579711c49c781c0f91931d86eb6"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240127_204215-cbyfcyz1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sagarsethi2503/huggingface/runs/cbyfcyz1' target=\"_blank\">upbeat-plasma-15</a></strong> to <a href='https://wandb.ai/sagarsethi2503/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sagarsethi2503/huggingface' target=\"_blank\">https://wandb.ai/sagarsethi2503/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sagarsethi2503/huggingface/runs/cbyfcyz1' target=\"_blank\">https://wandb.ai/sagarsethi2503/huggingface/runs/cbyfcyz1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:43, Epoch 100/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('./my_tokenizer/tokenizer_config.json',\n './my_tokenizer/special_tokens_map.json',\n './my_tokenizer/vocab.json',\n './my_tokenizer/merges.txt',\n './my_tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer,GenerationConfig\n\ndef generate_response(question, model, tokenizer, max_length=100):\n    # Tokenize the input question\n    \n    model.config.pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n    attention_mask = torch.ones_like(input_ids)\n    \n\n    # Generate response from the model\n    generation_config = GenerationConfig(\n    max_length=50,\n    temperature=1.2,\n    num_return_sequences=1\n    )\n\n    output = model.generate(input_ids,max_length=generation_config.max_length, temperature=generation_config.temperature)\n\n    # Decode the generated response\n    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_response\n\n# Load the fine-tuned GPT-2 model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/gpt2-finetuned\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/working/my_tokenizer\")\n\n# Example questions\nquestions = [\n    'Can you give me an example from history where the enemy was crushed totally from the book?',\n    'Whats the point of making myself less accessible?',\n    'Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?'\n    \n]\n\n# Generate responses to each question\nfor question in questions:\n    response = generate_response(question, model, tokenizer)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {response}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T21:04:14.517057Z","iopub.execute_input":"2024-01-27T21:04:14.517500Z","iopub.status.idle":"2024-01-27T21:04:25.020678Z","shell.execute_reply.started":"2024-01-27T21:04:14.517466Z","shell.execute_reply":"2024-01-27T21:04:25.019913Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Question: Can you give me an example from history where the enemy was crushed totally from the book?\nAnswer: Can you give me an example from history where the enemy was crushed totally from the book?\n\nI would say the French in the mid-nineteenth century were able to defeat the Ottoman Empire by throwing away their secular laws and replacing them with a\n\nQuestion: Whats the point of making myself less accessible?\nAnswer: Whats the point of making myself less accessible?\n\nI want to make myself more accessible to people who want to learn more about me and my work. I want to make it easier for people to learn more about me and my work by giving\n\nQuestion: Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?\nAnswer: Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?\n\nThe laws of power by robert greene penguin books hardcover edition isbn paperback edition isbn pages wisdom in a nutshellthe laws of power\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note:- Here as we increase our epochs and max outputs will show less repeated sentences and more variations till saturation, And  more repeated sentences are seen in case of less epochs.","metadata":{}},{"cell_type":"markdown","source":"# Loading the model for deploying","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport pickle\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Fine-tuned GPT-2 model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/gpt2-finetuned\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"/kaggle/working/my_tokenizer\")\n\n# Save model and tokenizer using pickle\nwith open(\"fine_tuned_model.pkl\", \"wb\") as f:\n    pickle.dump((model, tokenizer), f)\n\n# Load model and tokenizer from pickle file\nwith open(\"fine_tuned_model.pkl\", \"rb\") as f:\n    loaded_model, loaded_tokenizer = pickle.load(f)\n\n\ndef generate_response(question, model, tokenizer, max_length=100):\n    # Tokenize the input question\n    \n    model.config.pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(question, return_tensors=\"pt\")\n    attention_mask = torch.ones_like(input_ids)\n    \n\n    # Generate response from the model\n    generation_config = GenerationConfig(\n    max_length= max_length,\n    temperature=0.7,\n    num_return_sequences=1\n    )\n\n    output = model.generate(input_ids,max_length=generation_config.max_length, temperature=generation_config.temperature)\n\n    # Decode the generated response\n    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_response\n\n    \nquestions = [\n    'Can you give me an example from history where the enemy was crushed totally from the book?',\n    'Whats the point of making myself less accessible?',\n    'Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?'\n    \n]\nfrom transformers import GenerationConfig\nfor question in questions:\n    response = generate_response(question,loaded_model, loaded_tokenizer, max_length=500)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {response}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:44:45.769671Z","iopub.execute_input":"2024-01-27T20:44:45.770222Z","iopub.status.idle":"2024-01-27T20:47:10.011845Z","shell.execute_reply.started":"2024-01-27T20:44:45.770187Z","shell.execute_reply":"2024-01-27T20:47:10.010339Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Question: Can you give me an example from history where the enemy was crushed totally from the book?\nAnswer: Can you give me an example from history where the enemy was crushed totally from the book?\n\nI would say the French in the mid-nineteenth century were able to defeat the Ottoman Empire by throwing away their secular laws and replacing them with a more liberal one. They then turned the tide of the war by throwing in with the Catholics and the Protestants.\n\nWhat do you think of the idea that the Koran is the word of God?\n\nI think it is a very clever way to get people to believe in a lie. The Koran is not a book of laws written by an omniscient being. It is a compilation of laws from many different cultures and different times. It is not a book of laws by a single prophet. It is a compilation of laws by many different people over a very long period of time.\n\nThe fact is that the laws of power are not written in stone. They are constantly evolving and changing over time. The laws of power are not written in stone either. They are constantly being changed by human beings who want to make a quick buck and get a quick rise out of a difficult situation.\n\nThe laws of power are not written in stone either. They are constantly being changed by human beings who want to make a quick buck and get a quick rise out of a difficult situation.\n\nWhat do you think of the idea that the Koran is the word of God?\n\nI think it is a very clever way to get people to believe in a lie. The Koran is not a book of laws written by an omniscient being. It is a compilation of laws from many different cultures and different times. It is not a book of laws by a single prophet. It is a compilation of laws by many different people over a very long period of time.\n\nWhat do you think of the idea that the Koran is the word of God?\n\nI think it is a very clever way to get people to believe in a lie. The Koran is not a book of laws written by an omniscient being. It is a compilation of laws from many different cultures and different times. It is not a book of laws by a single prophet. It is a compilation of laws by many different people over a very long period of time.\n\nWhat do you think of the idea that the Koran is the word of God?\n\nI think it is a very clever way to get people to believe in a lie\n\nQuestion: Whats the point of making myself less accessible?\nAnswer: Whats the point of making myself less accessible?\n\nI want to make myself more accessible to people who want to learn more about me and my work. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post.\n\nI want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my work by giving them a more comprehensive and entertaining overview of my life and career than I could give them by writing a blog post. I want to make it easier for people to learn more about me and my\n\nQuestion: Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?\nAnswer: Can you tell me the story of Queen Elizabeth I from this 48 laws of power book?\n\nThe laws of power by robert greene penguin books hardcover edition isbn paperback edition isbn pages wisdom in a nutshellthe laws of power page www bizsum com copyright busi nesssummaries com the big idea a comprehensive well researc hed synthesis of timeless phi losophies from machiavelli to suntzu as applied in real life si tuations by powerful figures in history such as queen elizabeth i and henry kissinger absorbing and entertaining this book lends business people a wealth of ideas on the subtle art of playing the power ga me exercising clever cunning and understanding human weaknesses whether it is in the boardroom at a power l unch or a cocktail party these laws will make you master of the game a nd give you the edge over your rivals never outshine the master transgression of the law finance minister fouquet unintentionally out shone his master king louis xiv making the king feel insecure by throwing a lavish party that would show off fouquet s connections cultivated manner and charm thinking this move would make him an indispensable asset to the king fouquet had actually offended his master who did not like the fact people were more charmed by hi s finance minister than by him the king found a convenient excuse to get rid of fouquet observance of the law galileo was clever in observing this law by gi ving glory to his patrons in order to solve his perennial problem of funding he dedicated his discovery of the moons of jupiter to the medicis since the royal symbol of the m edici family was the planet jupiter he then commissioned an emblem for them with each moon representing one of the sons who revolved around the patriarch the medici family became his major patron appointing him their official court mathematician and philosopher thereby giving him a more comfortable life and a steady salary wisdom in a nutshell present your ideas in such a manner th at they may be ascribed to your master or could be viewed as an echo of your master s thoughts if you are more intelligent than your master act as if you are not never take your position for granted never let favors you receive go to your head discreet flattery is much more powerful make it seem like you want to seek his expertise and advice never put too much trust in friends learn how to use enemies transgression of the law michael iii of the byzantine empire in t he mid ninth century a d placed\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Thank You**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
